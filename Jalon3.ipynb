{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "N5C0qxlRVnDa",
        "ffojAzz47qtD",
        "Olc_vWh14aBc",
        "FLG2LCX24igr",
        "Rhu81GgvVGd6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "N5C0qxlRVnDa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "hwUShkMdh4ry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71d48664-afc0-48f6-df7d-69c5bb5694f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "'/content/drive/MyDrive/ESEO/I3/S9/Langage_naturel/TP_issou/model' -> './model'\n",
            "'/content/drive/MyDrive/ESEO/I3/S9/Langage_naturel/TP_issou/vect' -> './vect'\n"
          ]
        }
      ],
      "source": [
        "### import docs pickle from drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "! cp -r --verbose '/content/drive/MyDrive/ESEO/I3/S9/Langage_naturel/TP_issou/model' .\n",
        "! cp -r --verbose '/content/drive/MyDrive/ESEO/I3/S9/Langage_naturel/TP_issou/vect' .\n",
        "\n",
        "MODEL_FILE = './model'\n",
        "VECT_FILE = './vect'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pk"
      ],
      "metadata": {
        "id": "_ePc7ecrm2_5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open docs pickle\n",
        "\n",
        "model = pk.load(open(MODEL_FILE, 'rb'))\n",
        "vect = pk.load(open(VECT_FILE, 'rb'))\n",
        "print(vect)"
      ],
      "metadata": {
        "id": "faDm5bN3nm3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cbd6161-c481-4117-b984-90a493a56184"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TfidfVectorizer(max_df=0.36, min_df=0.05,\n",
            "                stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
            "                                      'afterwards', 'again', 'against', 'all',\n",
            "                                      'almost', 'alone', 'along', 'already',\n",
            "                                      'also', 'although', 'always', 'am',\n",
            "                                      'among', 'amongst', 'amoungst', 'amount',\n",
            "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
            "                                      'anyone', 'anything', 'anyway',\n",
            "                                      'anywhere', ...}))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = [\"Textblob is kinda bad to use. I don't like sushi but the ambiance was ok\"]"
      ],
      "metadata": {
        "id": "xgqLdkG6tKUe"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polarité"
      ],
      "metadata": {
        "id": "ffojAzz47qtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "testimonial1 = TextBlob(str(TEXT))\n",
        "print(str(TEXT))\n",
        "print(testimonial1.sentiment.polarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reVAqacrqKXg",
        "outputId": "854e057e-c02e-412c-e4da-15b76658a033"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Textblob is kinda bad to use. I don't like sushi but the ambiance was ok\"]\n",
            "-0.09999999999999992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def negative(text):\n",
        "  negative = False\n",
        "  testimonial1 = TextBlob(text)\n",
        "  if testimonial1.sentiment.polarity < 0:\n",
        "    negative = True\n",
        "  return negative "
      ],
      "metadata": {
        "id": "uo2-VFf96TVG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative(\"wasn't very pretty today, so I will not come a day like today\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yej9ygDm7BzS",
        "outputId": "d83abcf3-5410-45de-c057-efc6bb106134"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prétraitement"
      ],
      "metadata": {
        "id": "Olc_vWh14aBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "!pip3 install contractions\n",
        "import contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjyd9dx_5zn6",
        "outputId": "585eb8c9-8f2a-4ade-e758-613ed4d9d3ae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.72)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    text_processed = \" \".join(tokenizer.tokenize(text))\n",
        "    return text_processed"
      ],
      "metadata": {
        "id": "xcpDDrvu4ncr"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_text(\"wasn't very pretty today, so I will not come a day like today\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fBt-c61g5RVt",
        "outputId": "86915c44-6f3b-413e-c78f-984af03f6d85"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wasn t very pretty today so I will not come a day like today'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \n",
        "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    lemmatized_text_list = list()\n",
        "    \n",
        "    for word, tag in tokens_tagged:\n",
        "        if tag.startswith('J'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'a')) # Lemmatise adjectives. Not doing anything since we remove all adjective\n",
        "        elif tag.startswith('V'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'v')) # Lemmatise verbs\n",
        "        elif tag.startswith('N'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'n')) # Lemmatise nouns\n",
        "        elif tag.startswith('R'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'r')) # Lemmatise adverbs\n",
        "        else:\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatisation\n",
        "    \n",
        "    return \" \".join(lemmatized_text_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7f5nOqA4x8b",
        "outputId": "0f3c91e6-6ea7-4295-c791-aa72a129c6f5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/language.py:1899: UserWarning: [W123] Argument disable with value ['parser', 'tagger', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
            "  config_value=config[\"nlp\"][key],\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lemmatize_text(\"The brown fox is quick and he is jumping over the lazy dog\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "iubH4ULO5NrT",
        "outputId": "0be8e15a-fa55-4229-c84b-ac7c53efbc9c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.12 s, sys: 114 ms, total: 2.24 s\n",
            "Wall time: 2.45 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown fox be quick and he be jump over the lazy dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    return \" \".join([word.lower() for word in text.split()])"
      ],
      "metadata": {
        "id": "EAEYAG2B5DJz"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_text(\"The brown fox is quick and he is jumping over the lazy dog\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lPJFsOJq5K0D",
        "outputId": "424cf5d4-5731-4eda-b1ae-fd07e400196d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the brown fox is quick and he is jumping over the lazy dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contraction_text(text):\n",
        "    return contractions.fix(text)"
      ],
      "metadata": {
        "id": "XwMvCiCI5Fcb"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_text(\"The brown fox isn't quick and he wouldn't jumping over the lazy dog\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vwr_l19J5JiL",
        "outputId": "041b6c93-899b-45e8-8544-c18c116e5cb3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown fox is not quick and he would not jumping over the lazy dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = ['not', 'no', 'never', 'nor', 'hardly', 'barely']\n",
        "negative_prefix = \"NOT_\""
      ],
      "metadata": {
        "id": "aFEfssn05UK8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negative_token(text):\n",
        "    tokens = text.split()\n",
        "    negative_idx = [i+1 for i in range(len(tokens)-1) if tokens[i] in negative_words]\n",
        "    for idx in negative_idx:\n",
        "        if idx < len(tokens):\n",
        "            tokens[idx]= negative_prefix + tokens[idx]\n",
        "    \n",
        "    tokens = [token for i,token in enumerate(tokens) if i+1 not in negative_idx]\n",
        "    \n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "FqmoTiFp5VI0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_negative_token(\"I will never do that again, because I will not be foolish again, there is no risk, never\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "joxHB7G85WK8",
        "outputId": "1109e4cf-5a0c-45f1-b5e6-ec5e23e51b6d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I will NOT_do that again, because I will NOT_be foolish again, there is NOT_risk, never'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    english_stopwords = stopwords.words(\"english\") + list(STOP_WORDS) + [\"tell\", \"restaurant\"]\n",
        "    \n",
        "    return \" \".join([word for word in text.split() if word not in english_stopwords])"
      ],
      "metadata": {
        "id": "jKWTr7TY5Xmk"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_stopwords(\"I will never do that again, because I will not be foolish again, there is no risk, never\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IE092_D85Y48",
        "outputId": "2e0fa002-99d5-48a8-e0ed-3e9f7888343b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I again, I foolish again, risk,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Préprocess final"
      ],
      "metadata": {
        "id": "_obHuJfd5b08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = tokenize_text(text)\n",
        "    text = lemmatize_text(text)\n",
        "    text = normalize_text(text)\n",
        "    text = contraction_text(text)\n",
        "    text = get_negative_token(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "oxedCb5r5bN8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test = preprocess_text(\"I will never do that again, because I will not be foolish again, there is no risk, never\")\n",
        "print(text_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU2B4zw968_7",
        "outputId": "dc17def8-7af7-4e78-adce-0d8c7e2a8106"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOT_do NOT_be foolish NOT_risk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vectorization"
      ],
      "metadata": {
        "id": "FLG2LCX24igr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "my_vertorizer = vect.transform(TEXT)\n",
        "my_model = model.transform(my_vertorizer)\n",
        "\n",
        "print(np.argsort(my_model))\n",
        "print(my_model[0][0], my_model[0][-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq21xxX4r2Ge",
        "outputId": "3a7576ab-1e2d-4170-8b74-d55fa74a2759"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  5  6  9 13 14  2 10 12  3  7  8 11  4  0]]\n",
            "0.01652309329991388 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_nuns = ['atmosphere_sound', 'chicken_menu', 'bad_service', 'pizza_menu', 'delivery', 'long_wait', 'drinks', 'wrong_marketing', 'dirty', 'rude_staff', 'burger_menu', 'over_priced', 'not_tasty', 'not_accessible', 'seasoning']\n",
        "nb_topic = 2"
      ],
      "metadata": {
        "id": "hNgPywdyAqsL"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.argsort(my_model)) #list des topic les plus probable dans l'ordre décroissant\n",
        "print(my_model[0][0], my_model[0][-1]) #comparaison du dernier(0) et du premier(-1)\n",
        "for i in np.argsort(my_model): #inversement\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDJsr_WBBUba",
        "outputId": "34b171fb-9bfd-4bb9-fdfc-e43d5aa090b1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  5  6  9 13 14  2 10 12  3  7  8 11  4  0]]\n",
            "0.01652309329991388 0.0\n",
            "[ 1  5  6  9 13 14  2 10 12  3  7  8 11  4  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = pk.load(open(MODEL_FILE, 'rb'))\n",
        "vect = pk.load(open(VECT_FILE, 'rb'))\n",
        "\n",
        "def topic_search(text, nb_topic, model=model, vectorizer=vect):\n",
        "  my_vertorizer = vect.transform(text)\n",
        "  my_model = model.transform(my_vertorizer)\n",
        "  topics = []\n",
        "  for i in np.argsort(my_model):\n",
        "    for j in range(nb_topic):\n",
        "      topics.append(topic_nuns[i[-j-1]])\n",
        "  return topics"
      ],
      "metadata": {
        "id": "VlfdaAsNEKsi"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(topic_search(TEXT, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7womXx8uFF8A",
        "outputId": "510678b7-814c-4a65-bb95-f17076070b27"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['atmosphere_sound', 'delivery', 'over_priced']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation en fichier .py"
      ],
      "metadata": {
        "id": "Rhu81GgvVGd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/ESEO/I3/S9/Langage_naturel/TP_issou/appp.py\n",
        "\n",
        "MODEL_FILE = './model'\n",
        "VECT_FILE = './vect'\n",
        "\n",
        "\n",
        "#polarite\n",
        "\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "def negative(text):\n",
        "  negative = False\n",
        "  testimonial1 = TextBlob(text)\n",
        "  if testimonial1.sentiment.polarity < 0:\n",
        "    negative = True\n",
        "  return negative \n",
        "\n",
        "\n",
        "#pretraitement\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import contractions\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    text_processed = \" \".join(tokenizer.tokenize(text))\n",
        "    return text_processed\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    lemmatized_text_list = list()\n",
        "    for word, tag in tokens_tagged:\n",
        "        if tag.startswith('J'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'a')) # Lemmatise adjectives. Not doing anything since we remove all adjective\n",
        "        elif tag.startswith('V'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'v')) # Lemmatise verbs\n",
        "        elif tag.startswith('N'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'n')) # Lemmatise nouns\n",
        "        elif tag.startswith('R'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'r')) # Lemmatise adverbs\n",
        "        else:\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatisation\n",
        "    return \" \".join(lemmatized_text_list)\n",
        "\n",
        "def normalize_text(text):\n",
        "    return \" \".join([word.lower() for word in text.split()])\n",
        "\n",
        "def contraction_text(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "negative_words = ['not', 'no', 'never', 'nor', 'hardly', 'barely']\n",
        "negative_prefix = \"NOT_\"\n",
        "\n",
        "def get_negative_token(text):\n",
        "    tokens = text.split()\n",
        "    negative_idx = [i+1 for i in range(len(tokens)-1) if tokens[i] in negative_words]\n",
        "    for idx in negative_idx:\n",
        "        if idx < len(tokens):\n",
        "            tokens[idx]= negative_prefix + tokens[idx]\n",
        "    \n",
        "    tokens = [token for i,token in enumerate(tokens) if i+1 not in negative_idx]\n",
        "    \n",
        "    return \" \".join(tokens)\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    english_stopwords = stopwords.words(\"english\") + list(STOP_WORDS) + [\"tell\", \"restaurant\"]\n",
        "    \n",
        "    return \" \".join([word for word in text.split() if word not in english_stopwords])\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = tokenize_text(text)\n",
        "    text = lemmatize_text(text)\n",
        "    text = normalize_text(text)\n",
        "    text = contraction_text(text)\n",
        "    text = get_negative_token(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "#vectorization\n",
        "\n",
        "\n",
        "import pickle as pk\n",
        "import numpy as np\n",
        "\n",
        "topic_nuns = ['atmosphere_sound', 'chicken_menu', 'bad_service', 'pizza_menu', 'delivery', 'long_wait', 'drinks', 'wrong_marketing', 'dirty', 'rude_staff', 'burger_menu', 'over_priced', 'not_tasty', 'not_accessible', 'seasoning']\n",
        "\n",
        "  model = pk.load(open(MODEL_FILE, 'rb'))\n",
        "  vect = pk.load(open(VECT_FILE, 'rb'))\n",
        "\n",
        "def topic_search(text, nb_topic, model=model, vectorizer=vect):\n",
        "  my_vertorizer = vect.transform(text)\n",
        "  my_model = model.transform(my_vertorizer)\n",
        "  topics = []\n",
        "  for i in np.argsort(my_model):\n",
        "    for j in range(nb_topic):\n",
        "      topics.append(topic_nuns[i[-j-1]])\n",
        "  return topics\n",
        "\n"
      ],
      "metadata": {
        "id": "tNgfo-ldVKS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9aaaeff-d551-418a-fe64-27f3aca4f249"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/ESEO/I3/S9/Langage_naturel/TP_issou/appp.py\n"
          ]
        }
      ]
    }
  ]
}